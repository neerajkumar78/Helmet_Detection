{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install object-detection-fastai","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nfrom tqdm.notebook import tqdm\nfrom xml.etree.ElementTree import parse\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom fastai.vision import *\nfrom fastai import *\nfrom fastai.callbacks import *\nfrom sklearn.model_selection import StratifiedKFold,KFold\n\nfrom object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/hard-hat-detection/annotations'\npath_img = '../input/hard-hat-detection/images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_xml = []\nfor file in os.listdir(path):\n    if '.xml' in file:\n        file_xml.append(file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing xml files :\n* For **BBOXs** and **Labels**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hat = []\nlabels = []\nobject_xmin=[]\nobject_ymin=[]\nobject_xmax=[]\nobject_ymax=[]\nfor i in tqdm(range(len(file_xml))):\n    name = file_xml[i]\n    hat.append(name[:-4])\n    objects = parse(os.path.join(path,file_xml[i])).findall('object')\n    object_xmin.append([int(x.find(\"bndbox\").findtext(\"xmin\")) for x in objects])\n    object_ymin.append([int(x.find(\"bndbox\").findtext(\"ymin\")) for x in objects])\n    object_xmax.append([int(x.find(\"bndbox\").findtext(\"xmax\")) for x in objects])\n    object_ymax.append([int(x.find(\"bndbox\").findtext(\"ymax\")) for x in objects])\n    labels.append([x.findtext('name') for x in objects])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'file_name':hat,'xmin':object_xmin,'ymin':object_ymin,\n                                   'xmax':object_xmax,'ymax':object_ymax,'labels':labels})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path1 = Path('../input/hard-hat-detection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function to get BBOXs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_lbl(df):\n    hat2bbox = {}\n    for i in tqdm(range(df.shape[0])):\n        bbox = []\n        lbl =[]\n        title = []\n        a = df.iloc[i][1:-1].values\n        l = df.iloc[i][-1]\n        for j in range(len(l)):\n            bbx = [x[j] for x in a]\n            if l[j]!='person':\n                bbx = [bbx[1],bbx[0],bbx[3],bbx[2]]\n                lbl.append(bbx)\n                title.append(l[j])\n        bbox.append(lbl)\n        bbox.append(title)\n        hat2bbox[df.iloc[i][0]+'.png'] = bbox\n    return hat2bbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hat2bbox = image_lbl(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_sam(n):\n    name = df.iloc[n][0] + '.png'\n    fig,ax = plt.subplots(figsize=(8,8))\n    ax.imshow(immg.imread(os.path.join(path_img,name)))\n    B = hat2bbox[name]\n    for l,bbox in zip(B[1],B[0]):\n        bbox = [bbox[1],bbox[0],bbox[3],bbox[2]]\n        bbox[2] = abs(bbox[0]-bbox[2])\n        bbox[3] = abs(bbox[1]-bbox[3])\n        draw_rect(ax,bbox,text=l)\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_sam(random.randint(2,4800))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function to get bbox for each image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_y_func = lambda o: hat2bbox[Path(o).name] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms()\nsize = 512","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataBunch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (ObjectItemList.from_df(df,path1, folder = 'images' , suffix = '.png',cols='file_name')\n        #Where are the images? ->\n        .split_by_rand_pct(0.2)                          \n        #How to split in train/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=size,tfm_y=True)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=8, collate_fn=bb_pad_collate))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=2,  figsize=(15,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.train_ds),len(data.valid_ds),data.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anchors = create_anchors(sizes=[(32,32)], ratios=[1], scales=[0.3, 0.6, 1.2, 2, 2.8, 3.4,])\n#anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.4, 0.6, 0.85, 1, 1.6, 1.8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,8))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:6]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size / 2 \n    y = (bb[1] + 1) * size / 2 \n    w = bb[2] * size / 2\n    h = bb[3] * size / 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(anchors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet18, True, -2)\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=6, sizes=[32], chs=32, final_bias=-4., n_conv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data, model, \n                loss_func=crit, \n                callback_fns=[BBMetrics],\n                metrics=[voc],\n               model_dir='/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.split([model.encoder[6], model.c5top5])\nlearn.freeze_to(-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.lr_find()\n#learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(3, 1e-3 , callbacks = [ SaveModelCallback(learn, every ='improvement', monitor = 'AP-helmet', name = 'best_model' ) ] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('best_model');\nlearn.export('/kaggle/working/safetyHelmet.pkl');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3, callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='AP-helmet', name ='best_model_ft')] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('best_model_ft');\nlearn.export('/kaggle/working/safetyHelmet_ft.pkl');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_results_side_by_side(learn, anchors, detect_thresh=0.5, nms_thresh=0.1, image_count=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}